@MANUAL{bossy-2012bb,
  title = {Bacteria Biotope (BB) task at {BioNLP} Shared Task 2013. Task proposal},
  author = {Robert Bossy and Claire N\'edellec and Julien Jourde},
  organization = {INRA},
  address = {Jouy-en-Josas, France},
  year = {2012},
  owner = {grouin},
  timestamp = {2013.01.20},
  url = {http://2013.bionlp-st.org/tasks/}
}

@ARTICLE{bretonnel-2008plos,
  author = {Kevin Bretonnel-Cohen and Lawrence Hunter},
  title = {Getting Started in Text Mining},
  journal = {PLoS Comput Biol},
  year = {2008},
  volume = {4},
  pages = {e20},
  number = {1},
  abstract = {Text mining is the use of automated methods for exploiting the enormous
	amount of knowledge available in the biomedical literature. There
	are at least as many motivations for doing text mining work as there
	are types of bioscientists. Two more common (and more sophisticated)
	approaches to text mining exist: rule-based or knowledge-based approaches,
	and statistical or machine-learning-based approaches. The variety
	of types of rule-based systems is quite wide. In general, rule-based
	systems make use of some sort of knowledge. In contrast, statistical
	or machine-learning-based systems operate by building classifiers
	that may operate on any level, from labelling part of speech to choosing
	syntactic parse trees to classifying full sentences or documents.},
  owner = {grouin},
  timestamp = {2014.03.28}
}

@ARTICLE{brown-1992cl,
  author = {Peter F Brown and Vincent J {Della Pietra} and Peter V {de Souza}
	and Jenifer C Lai and Robert L Mercer},
  title = {Class-Based n-gram Models of Natural Language},
  journal = {Computational Linguistics},
  year = {1992},
  volume = {18},
  pages = {467--79},
  number = {4},
  abstract = {We address the problem of predicting a word from previous words in
	a sample of text. In particular, we discuss n-gram models based on
	classes of words. We also discuss several statistical algorithms
	for assigning words to classes based on the frequency of their co-occurrence
	with other words. We find that we are able to extract classes that
	have the flavor of either syntactically based groupings or semantically
	based groupings, depending on the nature of the underlying statistics.},
  owner = {grouin},
  review = {Algorithme de clustering non-supervisé fondé sur l'information mutuelle
	; regroupement de termes partageant des contextes gauches et droits
	(trigrammes de tokens) : "some words are similar to other words in
	their meaning and syntactic function" (p. 470). "the probability
	distribution of words in the vicinity of Thursday is very much like
	that for words in the vicinity of Friday" (p. 470).
	
	
	"We know of no practical method for finding one of the partitions
	that maximize the average mutual information." (p. 472).},
  timestamp = {2012.07.23}
}

@PHDTHESIS{ehrmann2008phd,
  author = {Maud Ehrmann},
  title = {Les entités nommées de la linguistique au TAL : statut théorique
	et méthodes de désambiguïsation},
  school = {Université Paris VII - Denis Diderot},
  year = {2008},
  abstract = {Le traitement des entités nommées fait aujourd'hui figure d'incontournable
	en Traitement Automatique des Langues. Apparue au milieu des années
	1990 à la faveur des dernières conférences MUC (Message Understanding
	Conferences), la tâche de reconnaissance et de catégorisation des
	noms de personnes, de lieux, d'organisations, etc. apparaît en effet
	comme fondamentale pour diverses applications participant de l'analyse
	de contenu et nombreux sont les travaux se consacrant à sa mise en
	oeuvre, obtenant des résultats plus qu'honorables. Fort de ce succès,
	le traitement des entités nommées s'oriente désormais vers de nouvelles
	perspectives avec, entre autres, la désambiguïsation et une annotation
	enrichie de ces unités. Ces nouveaux défis rendent cependant d'autant
	plus cruciale la question du statut théorique des entités nommées,
	lequel n'a guère été discuté jusqu'à aujourd'hui.
	
	Deux axes de recherche ont par conséquent été investis durant ce travail
	de thèse : nous avons, d'une part, tenté de proposer une définition
	des entités nommées à la suite et, d'autre part, expérimenté des
	méthodes de désambiguïsation. A la suite d'un état des lieux de la
	tâche de reconnaissance de ces unités et d'un exposé des difficultés
	pouvant se présenter à l'occasion d'une telle entreprise, il fut
	avant tout nécessaire d'examiner, d'un point de vue méthodologique,
	comment aborder la question de la définition des entités nommées.
	La démarche adoptée invita à se tourner du côté de la linguistique,
	avec les noms propres et les descriptions définies, puis du côté
	du traitement automatique, ce parcours visant au final à proposer
	une définition tenant compte tant des aspects du langage que des
	capacités et exigences des systèmes informatiques. La suite du mémoire
	rend compte d'un travail davantage expérimental, avec l'exposé d'une
	méthode d'annotation fine tout d'abord, de résolution de métonymie
	enfin. Ces travaux, combinant approche symbolique et approche distributionnelle,
	rendent compte de la possibilité d'une double annotation (catégories
	générales et catégories fines) et d'une désambiguïsation des entités
	nommées.},
  owner = {grouin},
  timestamp = {2012.08.30}
}

@ARTICLE{grouin-2014jbi,
  author = {Cyril Grouin and Aur\'elie N\'ev\'eol},
  title = {De-Identification of Clinical Notes in {French:} towards a Protocol
	for Reference Corpus Development},
  journal = {J Biomed Inform},
  year = {2014},
  volume = {50},
  pages = {151--61},
  abstract = {Background: To facilitate research applying Natural Language Processing
	to clinical documents, tools and resources are needed for the automatic
	de-identification of Electronic Health Records.
	
	Objective: This study investigates methods for developing a high-quality
	reference corpus for the de- identification of clinical documents
	in French.
	
	Methods: A corpus comprising a variety of clinical document types
	covering several medical specialties was pre-processed with two automatic
	de-identification systems from the MEDINA suite of tools: a rule-based
	system and a system using Conditional Random Fields (CRF). The pre-annotated
	documents were revised by two human annotators trained to mark ten
	categories of Protected Health Information (PHI). The human annotators
	worked independently and were blind to the system that produced the
	pre-annotations they were revising. The best pre-annotation system
	was applied to another random selection of 100 documents. After revision
	by one annotator, this set was used to train a statistical de- identification
	system.
	
	Results: Two gold standard sets of 100 documents were created based
	on the consensus of two human revisions of the automatic pre-annotations.
	The annotation experiment showed that (i) automatic pre- annotation
	obtained with the rule-based system performed better (F = 0.813)
	than the CRF system (F = 0.519), (ii) the human annotators spent
	more time revising the pre-annotations obtained with the rule-based
	system (from 102 to 160 minutes for 50 documents), compared to the
	CRF system (from 93 to 142 minutes for 50 documents), (iii) the quality
	of human annotation is higher when pre-annotations are obtained with
	the rule-based system (F-measure ranging from 0.970 to 0.987), compared
	to the CRF system (F-measure ranging from 0.914 to 0.981). Finally,
	only 20 documents from the training set were needed for the statistical
	system to outperform the pre-annotation systems that were trained
	on corpora from a medical speciality and hospital different from
	those in the reference corpus developed herein. Conclusion: We find
	that better pre-annotations increase the quality of the reference
	corpus but require more revision time. A statistical de-identification
	method outperforms our rule-based system when as little as 20 custom
	training documents are available.},
  comment = {PMID: 24380818},
  doi = {10.1016/j.jbi.2013.12.014},
  keywords = {Confidentiality; Electronic Health Records; France; Information Dissemination;
	Natural Language Processing},
  owner = {grouin},
  timestamp = {2014.02.14}
}

@INPROCEEDINGS{lafferty-2001icml,
  author = {John D. Lafferty and Andrew McCallum and Fernando C. N. Pereira},
  title = {Conditional {R}andom {F}ields: Probabilistic models for segmenting
	and labeling sequence data},
  booktitle = {Proc of ICML},
  year = {2001},
  pages = {282--9},
  address = {Williamstown, MA},
  abstract = {We present conditional random fields, a framework for building probabilistic
	models to segment and label sequence data. Conditional random fields
	offer several advantages over hidden Markov models and stochastic
	grammars for such tasks, including the ability to relax strong independence
	assumptions made in those models. Conditional random fields also
	avoid a fundamental limitation of maximum entropy Markov models (MEMMs)
	and other discriminative Markov models based on directed graphical
	models, which can be biased towards states with few successor states.
	We present iterative parameter estimation algorithms for conditional
	random fields and compare the performance of the resulting models
	to HMMs and MEMMs on synthetic and natural-language data.},
  owner = {grouin},
  review = {Introduction des CRF.},
  timestamp = {2012.09.06}
}

@INPROCEEDINGS{lavergne-2010acl,
  author = {Thomas Lavergne and Olivier Capp\'e and Fran\c{c}ois Yvon},
  title = {Practical Very Large Scale {CRFs}},
  booktitle = {Proc of ACL},
  year = {2010},
  pages = {504--13},
  address = {Uppsala, Sweden},
  month = {July},
  abstract = {Conditional Random Fields (CRFs) are a widely-used approach for supervised
	sequence labelling, notably due to their ability to handle large
	description spaces and to integrate structural dependency between
	labels. Even for the simple linear-chain model, taking structure
	into account implies a number of parameters and a computational effort
	that grows quadratically with the cardinality of the label set. In
	this paper, we address the issue of training very large CRFs, containing
	up to hundreds output labels and several billion features. Efficiency
	stems here from the sparsity induced by the use of a L1 penalty term.
	Based on our own implementation, we compare three recent proposals
	for implementing this regularization strategy. Our experiments demonstrate
	that very large CRFs can be trained efficiently and that very large
	models are able to improve the accuracy, while delivering compact
	parameter sets.},
  journal = {Proc of ACL},
  owner = {grouin},
  timestamp = {2012.12.24}
}

@BOOK{sekine-2009,
  title = {Named Entities},
  publisher = {John Benjamins Publishing},
  year = {2009},
  editor = {Satoshi Sekine and Elisabete Ranchhod},
  owner = {grouin},
  timestamp = {2012.09.05}
}
